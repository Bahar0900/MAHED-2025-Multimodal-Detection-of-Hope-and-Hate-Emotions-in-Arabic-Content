{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12532788,"sourceType":"datasetVersion","datasetId":7911671},{"sourceId":12536081,"sourceType":"datasetVersion","datasetId":7914077}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:19:21.512119Z","iopub.execute_input":"2025-07-21T17:19:21.512462Z","iopub.status.idle":"2025-07-21T17:19:21.533161Z","shell.execute_reply.started":"2025-07-21T17:19:21.512437Z","shell.execute_reply":"2025-07-21T17:19:21.532502Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaneddataaaa/cleaned_trained_dataset.csv\n/kaggle/input/testtrain/train.csv\n/kaggle/input/testtrain/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install required packages\n!pip install transformers>=4.51.0 torch torchvision torchaudio accelerate bitsandbytes -q\n!pip install sentencepiece protobuf pandas -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:19:21.534565Z","iopub.execute_input":"2025-07-21T17:19:21.534827Z","iopub.status.idle":"2025-07-21T17:20:53.791429Z","shell.execute_reply.started":"2025-07-21T17:19:21.534803Z","shell.execute_reply":"2025-07-21T17:20:53.790615Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Global variables\nMODEL_NAME = \"Qwen/Qwen3-14B\"\nLABELS = [\"hope\", \"hate\", \"not_applicable\"]\ntokenizer = None\nmodel = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:20:53.792527Z","iopub.execute_input":"2025-07-21T17:20:53.793114Z","iopub.status.idle":"2025-07-21T17:20:53.797533Z","shell.execute_reply.started":"2025-07-21T17:20:53.793086Z","shell.execute_reply":"2025-07-21T17:20:53.796722Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Arabic Text Classifier using Qwen3-14B for Kaggle - Functional Version\n# Make sure to enable GPU (T4 x2 recommended) in Kaggle notebook settings\n\n\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport pandas as pd\nimport numpy as np\nimport gc\nimport warnings\nimport re\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport json\nwarnings.filterwarnings('ignore')\n\n\n\ndef check_gpu():\n    \"\"\"Check GPU availability and print info\"\"\"\n    print(f\"CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\ndef load_model():\n    \"\"\"Load Qwen3-14B with optimizations for Kaggle and Arabic text\"\"\"\n    global tokenizer, model\n    \n    print(\"Loading Qwen3-14B model for Arabic Text Classification...\")\n    \n    # Configure quantization for memory efficiency on Kaggle\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,\n        \n    )\n    \n    # Load model with quantization\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True\n    )\n    \n    print(\"Qwen3-14B model loaded successfully!\")\n    print_model_info()\n\ndef print_model_info():\n    \"\"\"Print model and memory information\"\"\"\n    print(f\"\\nModel Information:\")\n    print(f\"Model Name: {MODEL_NAME}\")\n    print(f\"Model Parameters: 14.8B (13.2B non-embedding)\")\n    print(f\"Context Length: 32,768 tokens\")\n    print(f\"Tokenizer Vocab Size: {len(tokenizer):,}\")\n    print(f\"Classification Labels: {', '.join(LABELS)}\")\n    \n    if torch.cuda.is_available():\n        print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n        print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n\ndef extract_classification(response):\n    \"\"\"Extract classification label from response text\"\"\"\n    \n    # Most specific patterns first - look for the actual Classification section\n    patterns = [\n        r'Classification:\\s*\\[\\s*(hope|hate|not_applicable)\\s*\\]',  # Classification: [not_applicable]\n        r'Classification:\\s*\\*\\s*(hope|hate|not_applicable)\\s*\\*',  # Classification: *hope*\n        r'Classification:\\s*(hope|hate|not_applicable)\\s*$',        # Classification: hope (end of line)\n        r'Classification:\\s*(hope|hate|not_applicable)',            # Classification: hope\n        r'Category:\\s*(hope|hate|not_applicable)',\n        r'Label:\\s*(hope|hate|not_applicable)',\n        r'Result:\\s*(hope|hate|not_applicable)',\n        r'Final.*?:\\s*(hope|hate|not_applicable)',\n    ]\n    \n    response_lower = response.lower()\n    \n    # Try patterns in order of preference - take the LAST occurrence\n    for pattern in patterns:\n        matches = list(re.finditer(pattern, response_lower, re.IGNORECASE | re.MULTILINE))\n        if matches:\n            # Take the last match to avoid analysis section\n            match = matches[-1]\n            label = match.group(1).lower()\n            if label in LABELS:\n                confidence = estimate_confidence(response, label)\n                return label, confidence\n    \n    # More conservative fallback - only look in the last part of response\n    response_lines = response_lower.split('\\n')\n    last_lines = response_lines[-3:]  # Only check last 3 lines\n    \n    for line in reversed(last_lines):\n        for label in LABELS:\n            if f' {label}' in line or f'{label} ' in line or f'{label}.' in line:\n                return label, 0.6\n    \n    # Final fallback - but exclude analysis/reasoning sections\n    label_counts = {}\n    # Split response and only count from classification section onwards\n    classification_start = response_lower.find('classification')\n    if classification_start != -1:\n        relevant_text = response_lower[classification_start:]\n    else:\n        relevant_text = response_lower[-200:]  # Only last 200 chars\n    \n    for label in LABELS:\n        count = len(re.findall(r'\\b' + label + r'\\b', relevant_text))\n        if count > 0:\n            label_counts[label] = count\n    \n    if label_counts:\n        predicted_label = max(label_counts, key=label_counts.get)\n        confidence = 0.4  # Lower confidence for fallback\n        return predicted_label, confidence\n    \n    # Default fallback\n    return \"not_applicable\", 0.3\n\ndef estimate_confidence(response, predicted_label):\n    \"\"\"Estimate confidence based on response characteristics\"\"\"\n    confidence_indicators = [\n        \"clearly\", \"definitely\", \"obviously\", \"certain\", \"confident\",\n        \"strong\", \"evident\", \"apparent\", \"明显\", \"确定\", \"肯定\"\n    ]\n    \n    uncertainty_indicators = [\n        \"might\", \"possibly\", \"perhaps\", \"maybe\", \"unclear\",\n        \"difficult\", \"ambiguous\", \"uncertain\", \"可能\", \"也许\", \"不确定\"\n    ]\n    \n    response_lower = response.lower()\n    \n    confidence_score = 0.7  # Base confidence\n    \n    # Increase confidence if confident language is used\n    for indicator in confidence_indicators:\n        if indicator in response_lower:\n            confidence_score += 0.1\n            break\n    \n    # Decrease confidence if uncertain language is used\n    for indicator in uncertainty_indicators:\n        if indicator in response_lower:\n            confidence_score -= 0.2\n            break\n    \n    # Ensure confidence is in valid range\n    confidence_score = max(0.1, min(0.95, confidence_score))\n    \n    return round(confidence_score, 2)\n\ndef classify_arabic_text(text, use_thinking_mode=False, max_new_tokens=8000):\n    \"\"\"\n    Classify Arabic text into hope/hate/not_applicable categories\n    \n    Args:\n        text (str): Arabic text to classify\n        use_thinking_mode (bool): Enable Qwen3's thinking capabilities\n        max_new_tokens (int): Maximum tokens to generate\n    \n    Returns:\n        tuple: (full_response, predicted_label, confidence, thinking_content)\n    \"\"\"\n    global tokenizer, model\n    \n    if tokenizer is None or model is None:\n        raise ValueError(\"Model not loaded. Please call load_model() first.\")\n    \n    # Format the conversation for Qwen3\n    messages = [\n            {\n        \"role\": \"user\",\n        \"content\": f\"\"\"Please classify the following Arabic text into one of these categories:\n\nCategories:\n1. \"hope\" - Text expressing *clear* hope, longing for positive change, admiration with intent, affection toward a person or idea, or constructive sentiment. Purely poetic descriptions or artistic metaphors should only be labeled \"hope\" if they convey **intentional optimism** or **emotional uplift**.\n2. \"hate\" - Text expressing hate, negativity, hostility, discrimination, or destructive sentiment — including coded language, sarcasm, sectarian slogans, or historically divisive terms. However, sarcasm or satire **without a clear hostile target** should not be misclassified as hate.\n3. \"not_applicable\" - Text that doesn't clearly fit into hope or hate categories, including neutral, factual, or purely aesthetic/poetic content **without clear hopeful or hateful intent**.\n\nInstructions:\n- Carefully analyze whether the **intent** behind the text expresses constructive or destructive sentiment.\n- Do not classify admiration or personal preference for athletes, celebrities, or teams as \"hope\" unless there is a broader social or emotional message of uplift, change, or optimism.\n- Do **not** label emotional or metaphorical language as \"hope\" unless there is a **clear purpose** or **intent** of expressing optimism, affection, or desire for positive change.\n- **Spiritual or emotional self-criticism**, such as expressing a personal struggle with faith or oneself (e.g., “my heart fights God”), should be treated as **not_applicable** unless it directly targets a religious group or belief with hostility.\n- **Sarcasm, irony, or meme language** (e.g., using emojis or exaggeration) should be **ignored** unless it clearly targets a group or person in a hostile or discriminatory way.\n\nComparison Between All labels:\nCompare each pair by how they differ in emotional tone, intent, and expression:\n\nneutral vs anger: Neutral is emotionless or factual; anger expresses irritation or hostility.\n\nneutral vs anticipation: Neutral lacks expectation; anticipation expresses eagerness or hope.\n\nneutral vs disgust: Neutral is objective; disgust reflects moral or physical aversion.\n\nneutral vs fear: Neutral is calm; fear expresses anxiety or threat.\n\nneutral vs joy: Neutral lacks emotion; joy is positive and energetic.\n\nneutral vs love: Neutral is indifferent; love expresses affection and emotional closeness.\n\nneutral vs optimism: Neutral is flat; optimism is hopeful about outcomes.\n\nneutral vs pessimism: Neutral is unbiased; pessimism expects negative results.\n\nneutral vs sadness: Neutral is unfeeling; sadness conveys sorrow or loss.\n\nneutral vs surprise: Neutral is steady; surprise shows sudden reaction or disbelief.\n\nneutral vs trust: Neutral is detached; trust reflects confidence or belief in reliability.\n\nanger vs anticipation: Anger is hostile and reactive; anticipation is hopeful and forward-looking.\n\nanger vs disgust: Anger reacts to provocation; disgust judges or rejects morally or physically.\n\nanger vs fear: Anger confronts; fear avoids or retreats.\n\nanger vs joy: Anger is negative and aggressive; joy is positive and uplifting.\n\nanger vs love: Anger pushes away; love pulls closer with affection.\n\nanger vs optimism: Anger is critical or frustrated; optimism expects positive outcomes.\n\nanger vs pessimism: Both can be negative, but anger is active frustration; pessimism is passive hopelessness.\n\nanger vs sadness: Anger is explosive; sadness is internalized and quiet.\n\nanger vs surprise: Anger is predictable from offense; surprise is reaction to unexpected events.\n\nanger vs trust: Anger breaks bonds; trust builds them.\n\nanticipation vs disgust: Anticipation looks forward positively; disgust pushes away negatively.\n\nanticipation vs fear: Anticipation is hopeful expectation; fear is anxious expectation.\n\nanticipation vs joy: Anticipation is future-focused; joy is in the present.\n\nanticipation vs love: Anticipation expects; love connects emotionally.\n\nanticipation vs optimism: Anticipation expects a specific event; optimism is a general positive outlook.\n\nanticipation vs pessimism: Anticipation is hopeful; pessimism is doubtful.\n\nanticipation vs sadness: Anticipation excites; sadness dulls.\n\nanticipation vs surprise: Anticipation expects; surprise is unexpected.\n\nanticipation vs trust: Anticipation is forward-looking; trust is present confidence.\ndisgust vs fear: Disgust is moral or physical rejection; fear is about danger or harm.\n\ndisgust vs joy: Disgust is repelled; joy is attracted.\n\ndisgust vs love: Disgust rejects; love embraces.\n\ndisgust vs optimism: Disgust is negative evaluation; optimism is positive expectation.\n\ndisgust vs pessimism: Both are negative; disgust is reactive judgment; pessimism is negative belief.\n\ndisgust vs sadness: Disgust rejects others; sadness focuses on inner loss.\n\ndisgust vs surprise: Disgust is evaluative; surprise is reactive.\n\ndisgust vs trust: Disgust pushes away; trust builds closeness.\nfear vs joy: Fear is anxiety; joy is happiness.\n\nfear vs love: Fear is isolating; love is connecting.\n\nfear vs optimism: Fear expects harm; optimism expects good.\n\nfear vs pessimism: Both expect bad; fear is emotional and urgent, pessimism is mental and resigned.\n\nfear vs sadness: Fear relates to danger; sadness relates to loss.\n\nfear vs surprise: Fear is anticipating harm; surprise is sudden and undefined.\n\nfear vs trust: Fear hesitates; trust commits.\njoy vs love: Joy is delight in the moment; love is deep emotional attachment.\n\njoy vs optimism: Joy is current happiness; optimism is future hope.\n\njoy vs pessimism: Joy is positive; pessimism is negative.\n\njoy vs sadness: Joy and sadness are opposites in emotional tone.\n\njoy vs surprise: Joy is lasting; surprise is brief.\n\njoy vs trust: Joy is feeling good; trust is feeling safe.\nlove vs optimism: Love is relational; optimism is outlook-based.\n\nlove vs pessimism: Love is warm and connecting; pessimism is cold and distant.\n\nlove vs sadness: Love is bonding; sadness often reflects loss of love.\n\nlove vs surprise: Love is expected over time; surprise is sudden.\n\nlove vs trust: Love is emotional closeness; trust is reliability and belief.\n\noptimism vs pessimism: Optimism expects good; pessimism expects bad.\n\noptimism vs sadness: Optimism looks forward positively; sadness reflects current or past loss.\n\noptimism vs surprise: Optimism is long-term hope; surprise is momentary shock.\n\noptimism vs trust: Optimism is about outcomes; trust is about people or systems.\npessimism vs sadness: Pessimism expects bad; sadness feels bad.\n\npessimism vs surprise: Pessimism expects negative; surprise reacts to unexpected (positive or negative).\n\npessimism vs trust: Pessimism doubts; trust believes.\n\nsadness vs surprise: Sadness is deep and emotional; surprise is brief and reactive.\n\nsadness vs trust: Sadness reflects pain; trust reflects faith.\nsurprise vs trust: Surprise is sudden reaction; trust is steady belief.\nOffensive content includes insults, slurs, profanity, vulgar jokes, threats, sexually explicit or socially inappropriate language.\nNon-offensive content is respectful, civil, and appropriate—even if emotionally intense (e.g., sadness, fear, or anger), it avoids harmful or vulgar expression.\n\nHate content targets identity groups (e.g., race, religion, nationality, gender, sexuality) with derogatory, dehumanizing, or violent language.\nNot_hate content may still be offensive (e.g., personal insults, rude jokes, spam, crude humor), but does not target individuals or groups based on identity.\n\n\nArabic Text: {text}\n\nProvide your response in this format:\n- Analysis: [Explain what the text is saying and whether it shows hostility, admiration, or neutrality]\n- Reasoning: [Explain your classification decision]\n- Classification: [hope/hate/not_applicable]\n\"\"\"\n\n    }\n    ]\n\n    \n    # Apply chat template with thinking mode control\n    text_input = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=use_thinking_mode\n    )\n    \n    # Tokenize input\n    model_inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n    \n    # Generate response with optimized parameters for Qwen3\n    with torch.no_grad():\n        if use_thinking_mode:\n            # Thinking mode parameters\n            generated_ids = model.generate(\n                **model_inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.6,\n                top_p=0.7,\n                top_k=20,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n        else:\n            # Non-thinking mode parameters\n            generated_ids = model.generate(\n                **model_inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=0.7,\n                top_p=0.8,\n                top_k=20,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                repetition_penalty=1.1\n            )\n    \n    # Extract output tokens\n    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n    \n    # Parse thinking content (if thinking mode is enabled)\n    thinking_content = \"\"\n    content = \"\"\n    \n    if use_thinking_mode:\n        try:\n            # Find </think> token (151668)\n            index = len(output_ids) - output_ids[::-1].index(151668)\n            thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\n            content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n        except ValueError:\n            # No thinking tags found\n            content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n    else:\n        content = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n    \n    # Extract the classification and confidence\n    predicted_label, confidence = extract_classification(content)\n    \n    return content, predicted_label, confidence, thinking_content\n\ndef cleanup_memory():\n    \"\"\"Clean up GP memory\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\ndef process_dataset(df, text_column='text', id_column='id', \n                   use_thinking=True, batch_size=10, save_progress=True):\n    \"\"\"\n    Process entire dataset for classification\n    \n    Args:\n        df (pd.DataFrame): Dataset with Arabic text\n        text_column (str): Name of text column\n        id_column (str): Name of ID column  \n        use_thinking (bool): Enable thinking mode\n        batch_size (int): Process in batches to manage memory\n        save_progress (bool): Save intermediate results\n    \n    Returns:\n        pd.DataFrame: Results with predictions\n    \"\"\"\n    \n    results = []\n    total_samples = len(df)\n    \n    print(f\"🚀 Processing {total_samples} Arabic text samples...\")\n    print(f\"📊 Batch size: {batch_size}\")\n    print(\"-\" * 60)\n    \n    for i in range(0, total_samples, batch_size):\n        batch_end = min(i + batch_size, total_samples)\n        batch_df = df.iloc[i:batch_end]\n        \n        print(f\"Processing batch {i//batch_size + 1}/{(total_samples-1)//batch_size + 1} (samples {i+1}-{batch_end})\")\n        \n        batch_results = []\n        \n        for idx, row in batch_df.iterrows():\n            try:\n                text = row[text_column]\n                sample_id = row[id_column]\n                \n                # Classify the text\n                response, predicted_label, confidence, thinking = classify_arabic_text(\n                    text, use_thinking_mode=use_thinking, max_new_tokens=1500\n                )\n                \n                result = {\n                    'id': sample_id,\n                    'text': text,\n                    'predicted_label': predicted_label,\n                    'confidence': confidence,\n                    'full_response': response,\n                    'thinking_process': thinking if use_thinking else \"\"\n                }\n                \n                # Add actual label if exists\n                if 'label' in row:\n                    result['actual_label'] = row['label']\n                \n                batch_results.append(result)\n                print(f\"  ✅ Sample {sample_id}: {predicted_label} (confidence: {confidence})\")\n                \n            except Exception as e:\n                print(f\"  ❌ Error processing sample {row[id_column]}: {str(e)}\")\n                batch_results.append({\n                    'id': row[id_column],\n                    'text': row[text_column],\n                    'predicted_label': 'not_applicable',\n                    'confidence': 0.1,\n                    'full_response': f\"Error: {str(e)}\",\n                    'thinking_process': \"\"\n                })\n        \n        results.extend(batch_results)\n        \n        # Clean up memory after each batch\n        cleanup_memory()\n        \n        # Save intermediate progress\n        if save_progress and (i + batch_size) % (batch_size * 5) == 0:\n            temp_df = pd.DataFrame(results)\n            temp_df.to_csv(f'intermediate_results_batch_{i//batch_size + 1}.csv', index=False)\n            print(f\"  💾 Saved intermediate results up to batch {i//batch_size + 1}\")\n        \n        print(f\"  ✅ Batch completed. Memory cleaned.\")\n        print(\"-\" * 40)\n    \n    results_df = pd.DataFrame(results)\n    \n    print(f\"🎉 Classification completed!\")\n    print(f\"📊 Total processed: {len(results_df)} samples\")\n    \n    return results_df\n\ndef evaluate_predictions(results_df):\n    \"\"\"Evaluate predictions if actual labels are available\"\"\"\n    \n    if 'actual_label' not in results_df.columns:\n        print(\"⚠️ No actual labels found for evaluation\")\n        return None\n    \n    # Filter out rows with missing actual labels\n    eval_df = results_df.dropna(subset=['actual_label'])\n    \n    if len(eval_df) == 0:\n        print(\"⚠️ No valid actual labels found for evaluation\")\n        return None\n    \n    y_true = eval_df['actual_label'].tolist()\n    y_pred = eval_df['predicted_label'].tolist()\n    \n    print(\"📊 Classification Report:\")\n    print(\"=\" * 60)\n    print(classification_report(y_true, y_pred, target_names=LABELS))\n    \n    print(\"\\n📊 Confusion Matrix:\")\n    print(\"=\" * 30)\n    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n    cm_df = pd.DataFrame(cm, index=LABELS, columns=LABELS)\n    print(cm_df)\n    \n    # Calculate accuracy by label\n    print(\"\\n📊 Accuracy by Label:\")\n    print(\"=\" * 30)\n    for label in LABELS:\n        label_mask = eval_df['actual_label'] == label\n        if label_mask.sum() > 0:\n            accuracy = (eval_df[label_mask]['predicted_label'] == label).mean()\n            print(f\"{label}: {accuracy:.3f} ({label_mask.sum()} samples)\")\n    \n    # Overall accuracy\n    overall_accuracy = (eval_df['actual_label'] == eval_df['predicted_label']).mean()\n    print(f\"\\nOverall Accuracy: {overall_accuracy:.3f}\")\n    \n    return {\n        'accuracy': overall_accuracy,\n        'classification_report': classification_report(y_true, y_pred, target_names=LABELS, output_dict=True),\n        'confusion_matrix': cm_df\n    }\n\ndef load_and_process_dataset(file_path, text_column='text', id_column='id', \n                           label_column='label', use_thinking=True):\n    \"\"\"\n    Load and process the Arabic dataset\n    \n    Args:\n        file_path (str): Path to CSV file\n        text_column (str): Name of text column\n        id_column (str): Name of ID column\n        label_column (str): Name of label column (optional)\n        use_thinking (bool): Enable thinking mode\n    \n    Returns:\n        tuple: (original_df, results_df, evaluation_results)\n    \"\"\"\n    \n    # Load dataset\n    print(f\"📂 Loading dataset from: {file_path}\")\n    df = pd.read_csv(file_path)\n    \n    print(f\"📊 Dataset shape: {df.shape}\")\n    print(f\"📋 Columns: {list(df.columns)}\")\n    \n    # Show sample data\n    print(\"\\n📝 Sample data:\")\n    print(df.head())\n    \n    # Check label distribution if available\n    if label_column in df.columns:\n        print(f\"\\n📊 Label distribution:\")\n        print(df[label_column].value_counts())\n    \n    # Process the dataset\n    results_df = process_dataset(\n        df, \n        text_column=text_column, \n        id_column=id_column, \n        use_thinking=use_thinking,\n        batch_size=5,  # Small batch size for memory management\n        save_progress=True\n    )\n    \n    # Evaluate if possible\n    evaluation_results = None\n    if label_column in df.columns:\n        results_df['actual_label'] = df[label_column]\n        evaluation_results = evaluate_predictions(results_df)\n    \n    return df, results_df, evaluation_results\n\ndef classify_single_text(text, show_details=True):\n    \"\"\"Classify a single Arabic text\"\"\"\n    \n    print(\"🔍 Single Text Classification\")\n    print(\"=\" * 50)\n    print(f\"Arabic Text: {text}\")\n    print(\"-\" * 50)\n    \n    response, predicted_label, confidence, thinking = classify_arabic_text(\n        text, use_thinking_mode=True\n    )\n    \n    if show_details and thinking:\n        print(\"🧠 Thinking Process:\")\n        print(thinking)\n        print(\"\\n\" + \"-\" * 40)\n    \n    print(\"📝 Analysis:\")\n    print(response)\n    \n    print(f\"\\n✅ Classification: {predicted_label}\")\n    print(f\"🎯 Confidence: {confidence}\")\n    print(\"=\" * 50)\n    \n    return predicted_label, confidence\n\ndef run_example_classifications():\n    \"\"\"Run example Arabic text classifications\"\"\"\n    \n    example_texts = [\n        \"أتمنى أن يكون الغد أفضل من اليوم\",  # Hope: I hope tomorrow will be better than today\n        \"أكره هذا الشيء كثيراً\",  # Hate: I hate this thing so much  \n        \"الطقس اليوم مشمس\",  # Not applicable: The weather is sunny today\n        \"سأحارب من أجل المستقبل الأفضل\",  # Hope: I will fight for a better future\n        \"هؤلاء الناس لا يستحقون الاحترام\",  # Hate: These people don't deserve respect\n        \"كم الساعة الآن؟\"  # Not applicable: What time is it now?\n    ]\n    \n    print(\"🚀 Running Example Arabic Text Classifications\")\n    print(\"=\" * 60)\n    \n    for i, text in enumerate(example_texts, 1):\n        print(f\"\\nExample {i}:\")\n        classify_single_text(text, show_details=False)\n\ndef save_results(results_df, filename=\"arabic_classification_results.csv\"):\n    \"\"\"Save classification results\"\"\"\n    results_df.to_csv(filename, index=False, encoding='utf-8')\n    print(f\"💾 Results saved to: {filename}\")\n    \n    # Also save a summary\n    summary = {\n        'total_samples': len(results_df),\n        'label_distribution': results_df['predicted_label'].value_counts().to_dict(),\n        'avg_confidence': results_df['confidence'].mean(),\n        'confidence_by_label': results_df.groupby('predicted_label')['confidence'].mean().to_dict()\n    }\n    \n    with open(filename.replace('.csv', '_summary.json'), 'w', encoding='utf-8') as f:\n        json.dump(summary, f, indent=2, ensure_ascii=False)\n    \n    print(f\"📊 Summary saved to: {filename.replace('.csv', '_summary.json')}\")\n\n# Initialize the system \nprint(\"🚀 Arabic Text Classifier (Functional Version) ready!\")\n\n# Check GPU and load model\ncheck_gpu()\nload_model()\n\nprint(\"📚 Running example classifications...\")\n\n# Example usage:\n# run_example_classifications()\n\n# To use with your dataset:\n# df, results_df, eval_results = load_and_process_dataset('your_dataset.csv')\n# save_results(results_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:20:53.799576Z","iopub.execute_input":"2025-07-21T17:20:53.799779Z","iopub.status.idle":"2025-07-21T17:27:47.291299Z","shell.execute_reply.started":"2025-07-21T17:20:53.799762Z","shell.execute_reply":"2025-07-21T17:27:47.290397Z"}},"outputs":[{"name":"stdout","text":"🚀 Arabic Text Classifier (Functional Version) ready!\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 15.9 GB\nLoading Qwen3-14B model for Arabic Text Classification...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8498ae2376ed49568fc969a8f6ede474"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd218d24d7d4417a87c1acfd94fde73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b421108019c4891993e35fd52d2e34e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41650c0428c746d09e5a705f2801475c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98414b462f04978906f597350be562d"}},"metadata":{}},{"name":"stderr","text":"2025-07-21 17:21:15.205907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753118475.402143      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753118475.456368      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d02ad81755498395292927a4840c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4a08e7208d4aafa7e1be5c41f6b3e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/3.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a7972d540245bcbdd07b2a57f63721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aab194ad129e41d48f65b5df565e629e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984f3be660374133b4f979623a80e115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ccf14822ed4c5b8aac70d161957ecf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be0f6d0194624ee3bcd0dc67d1405556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b27f733ed5e44f3baaa4e827d5d83e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc46a2c73124ee28101f64ec438e794"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/1.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56559086f4404b6592035d956cfa9eb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e0bb549623479d81c83f2722b89c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"731684205ae94896ab06d9019f56e848"}},"metadata":{}},{"name":"stdout","text":"Qwen3-14B model loaded successfully!\n\nModel Information:\nModel Name: Qwen/Qwen3-14B\nModel Parameters: 14.8B (13.2B non-embedding)\nContext Length: 32,768 tokens\nTokenizer Vocab Size: 151,669\nClassification Labels: hope, hate, not_applicable\nGPU Memory Allocated: 9.27 GB\nGPU Memory Reserved: 11.00 GB\n📚 Running example classifications...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"text='مختصر دين الوهابية حاجات معاوية_رأس_الفتنة'\nclassify_single_text(text, show_details=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:27:47.292325Z","iopub.execute_input":"2025-07-21T17:27:47.292952Z","iopub.status.idle":"2025-07-21T17:29:09.722023Z","shell.execute_reply.started":"2025-07-21T17:27:47.292923Z","shell.execute_reply":"2025-07-21T17:29:09.721308Z"}},"outputs":[{"name":"stdout","text":"🔍 Single Text Classification\n==================================================\nArabic Text: مختصر دين الوهابية حاجات معاوية_رأس_الفتنة\n--------------------------------------------------\n📝 Analysis:\n- Analysis: The text references \"Wahabi religion\" (الوهابية) and links it to Muawiya ibn Abi Sufyan (معاوية), a historical figure associated with the First Fitna (a period of civil strife in early Islamic history). The phrase \"رأس الفتنة\" (\"head of the Fitna\") carries a strongly negative connotation, implying blame for division or conflict. The text appears to criticize or delegitimize Wahhabism through its association with a controversial historical figure.  \n- Reasoning: The text explicitly connects a specific religious ideology (Wahhabism) to a historical figure (Muawiya) framed as a source of division (\"رأس الفتنة\"). This association implies hostility or disdain toward Wahhabism, aligning with the \"hate\" category due to its discriminatory or critical tone toward a religious group. While the phrasing is concise, the intent to associate a religion with a symbol of conflict suggests destructive sentiment rather than neutral or hopeful messaging.  \n- Classification: hate\n\n✅ Classification: hate\n🎯 Confidence: 0.8\n==================================================\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('hate', 0.8)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/testrain/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:46:55.522344Z","iopub.status.idle":"2025-07-21T17:46:55.523098Z","shell.execute_reply.started":"2025-07-21T17:46:55.522921Z","shell.execute_reply":"2025-07-21T17:46:55.522938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnt=0\n\nfor i in range(40):\n    text = df['text'][i]\n    prediction,confidence = classify_single_text(text, show_details=False)\n    print(prediction)\n    print(df['label'][i])\n    if prediction == df['label'][i]:\n        cnt+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:46:55.524183Z","iopub.status.idle":"2025-07-21T17:46:55.524608Z","shell.execute_reply.started":"2025-07-21T17:46:55.524433Z","shell.execute_reply":"2025-07-21T17:46:55.524449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(cnt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T17:46:55.525731Z","iopub.status.idle":"2025-07-21T17:46:55.526013Z","shell.execute_reply.started":"2025-07-21T17:46:55.525850Z","shell.execute_reply":"2025-07-21T17:46:55.525865Z"}},"outputs":[],"execution_count":null}]}